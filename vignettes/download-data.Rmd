---
title: "VDownloading Data"
author: "Joshua H. Cook"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r load_libs, message = FALSE, warning = FALSE}
library(magrittr)
library(dplyr)
library(stringr)
library(readr)
```

## Data Sources

BAM files were download from Genomic Data Commons (GDC): TCGA-COAD, TCGA-READ, TCGA-AML, TCGA-PAAD, TCGA-DLBC

Tumor purity scores were downloaded from https://gdc.cancer.gov/about-data/publications/pancanatlas and labeled "ABSOLUTE purity/ploidy file - TCGA_mastercalls.abs_tables_JSedit.fixed.txt"

Tumor CNV data were downloaded from https://gdc.cancer.gov/about-data/publications/pancanatlas and labeled "Copy Number - broad.mit.edu_PANCAN_Genome_Wide_SNP_6_whitelisted.seg"


## Prepare file name lists using manifest and sample sheet from GDC with "create_file_name_lists.R"

The Manifest and Sample Sheet were downloaded from GDC Data Portal after adding the files to the cart. They were save in "data".

```{r datafiles, eval = FALSE}
samplesheet_dir <- "data/tcga_sample_sheet.txt"
manifest <- "data/tcga_manifest.txt"
```

The sample sheet was loaded and filtered for the desired sample types.

```{r load_samplesheet, eval = FALSE}
samplesheet <- read_tsv(samplesheet_dir)
colnames(samplesheet) <- str_replace_all(colnames(samplesheet), " ", "_") %>%
    str_to_lower()

# Filter for primary tumors samples
samplesheet %<>%
    filter(sample_type %in% c("Primary Blood Derived Cancer - Bone Marrow",
                              "Primary Blood Derived Cancer - Peripheral Blood",
                              "Primary Tumor")) %>%
    filter(!str_detect(file_name, "gapfillers")) %>%
    unique()
samplesheet
```

### Keep only one data file per case ID

There were some case IDs with more than one file. I used a bucnh of manual filters to eventually only have one file per sample. You can see these filters in "R/download-createfilenamelists.R".

I began by gathering the IDs with multiple files.

```{r one_caseid, eval = FALSE}
ids_table <- table(samplesheet$case_id)
ids_table <- ids_table[ids_table > 1]
length(ids_table)
```

Only `\r length(ids_table)` samples had more than one file, so this was not a wide-spread problem.

```{r hangle_multisamples, eval = FALSE}
samplesheet <- samplesheet %>%
    group_by(case_id) %>%
    mutate(keep = choose_one_filename(file_name)) %>%
    filter(keep == file_name) %>%
    ungroup()

# check that all case_IDs are unique
uniqueN(samplesheet$case_id) == nrow(samplesheet)
```

The final list of filenames was saved to "data/tcga_filename_list.txt"

```{r save_filenamelist, eval = FALSE}
cat(unlist(samplesheet$file_id),
    file = "data/tcga_filename_list.txt",
    sep = "\n")
```

TODO: need to figure out organization strategy for data


# TODO:


## Download and processing with "download_process_tcga_bams.sh"

This script is submitted as a batch array, passing as the first argument a file listing the file names from GDC (see "create_file_name_lists.R"). Below is the step-by-step process:

1. the region surround KRAS is downloaded from the GDC API 
2. `samtools index` indexes the BAM file
3. `bcftools mpileup | bcftools call` makes a VCF of variants in KRAS
4. `bcftools mpileup` creates an annotated VCF
5. `annovar` annotates the VCF file to find mutations in KRAS

The script was run as a batch array using the following command

```bash
sbatch --array=1-$(wc -l < input/tcga_filename_list.txt)%50 download_process_tcga_bams.sh input/tcga_filename_list.txt
```

